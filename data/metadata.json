[
  {
    "id": "75c6335b380542a080da5c414050664f",
    "filename": "NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "original_name": "NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "filepath": "E:\\WEB_TASK\\uploads\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "size": 569417,
    "uploaded_at": "2025-12-26T02:16:20.110178",
    "analysis": {
      "_version": "2",
      "category": "",
      "summary": "Transformer是一种全新的神经网络架构，完全基于注意力机制，摒弃了传统的循环和卷积结构。  \n该模型在机器翻译任务中实现了更优的质量和更高的并行化效率，显著减少了训练时间。  \n它解决了循环神经网络顺序计算的固有限制，提升了长序列处理的性能。",
      "deep_read": "**背景/问题**  \n在Transformer提出之前，机器翻译等序列转换任务主要依赖循环神经网络（RNN）或卷积神经网络（CNN），这些模型通常需要按序列顺序逐步计算，导致训练速度慢、难以并行处理，尤其对长序列的建模能力有限。  \n\n**核心论点**  \n论文主张完全摒弃循环和卷积结构，仅依赖注意力机制来构建序列模型，即“注意力就是所需的一切”。由此提出的Transformer架构，通过自注意力机制直接捕捉序列中任意位置之间的关系，实现了更高效的并行计算与更强的长程依赖建模。  \n\n**关键证据**  \n在WMT 2014英德翻译任务中，Transformer取得了28.4 BLEU的分数，比当时最佳结果提升超过2 BLEU；在英法翻译任务中，仅用8块GPU训练3.5天就达到41.0 BLEU，刷新了单模型性能纪录。这些结果表明，Transformer在翻译质量上显著优于以往模型，同时训练时间大幅缩短。  \n\n**结论或启发**  \nTransformer证明了仅基于注意力机制的架构能够克服循环网络的顺序计算瓶颈，在保持高性能的同时实现高度并行化，极大提升了训练效率。这一设计为后续预训练模型（如BERT、GPT）奠定了基础，推动了自然语言处理领域的范式变革。",
      "translation": "**注意力机制即是所需一切**  \nAshish Vaswani  \nGoogle Brain  \navaswani@google.com  \nNoam Shazeer  \nGoogle Brain  \nnoam@google.com  \nNiki Parmar  \nGoogle Research  \nnikip@google.com  \nJakob Uszkoreit  \nGoogle Research  \nusz@google.com  \nLlion Jones  \nGoogle Research  \nllion@google.com  \nAidan N. Gomez  \nUniversity of Toronto  \naidan@cs.toronto.edu  \nŁukasz Kaiser  \nGoogle Brain  \nlukaszkaiser@google.com  \nIllia Polosukhin  \nillia.polosukhin@gmail.com  \n\n**摘要**  \n主流的序列转导模型基于复杂的循环神经网络或卷积神经网络，这些网络包含编码器与解码器。性能最佳的模型还通过注意力机制连接编码器和解码器。我们提出了一种新的简单网络架构——Transformer，它完全基于注意力机制，彻底摒弃了循环和卷积结构。在两个机器翻译任务上的实验表明，该模型在质量上更优，同时具有更高的可并行化能力，且训练所需时间显著减少。我们的模型在WMT 2014英德翻译任务上取得了28.4 BLEU分数，比现有最佳结果（包括集成模型）提高了超过2 BLEU。在WMT 2014英法翻译任务上，我们的模型在8个GPU上训练3.5天后，取得了41.0 BLEU的单模型最新最优成绩，这仅占文献中最佳模型训练成本的一小部分。  \n\n**1 引言**  \n循环神经网络，尤其是长短期记忆网络[12]和门控循环神经网络[7]，已被牢固确立为序列建模和转导问题（如语言建模和机器翻译[29,2,5]）中的先进方法。此后，众多研究持续推动循环语言模型及编码器-解码器架构的边界。\n\n架构[31, 21, 13]。  \n\u0003同等贡献。作者列表顺序随机。Jakob 提出用自注意力机制替换循环神经网络，并率先开展该理念的评估工作。Ashish 与 Illia 共同设计并实现了首个 Transformer 模型，且对本工作的各个方面都做出了关键贡献。Noam 提出了缩放点积注意力、多头注意力及无参数位置表示方法，并几乎参与了所有细节工作。Niki 在我们原始代码库和 tensor2tensor 中设计、实现、调优并评估了无数模型变体。Llion 同样尝试了新颖的模型变体，负责初始代码库开发，以及高效推理与可视化实现。Lukasz 和 Aidan 投入了大量时间设计 tensor2tensor 的各个组件并完成实现，取代了早期代码库，显著提升结果并极大加速了研究进程。  \ny工作完成于谷歌大脑任职期间。  \nz工作完成于谷歌研究院任职期间。  \n第31届神经信息处理系统大会（NIPS 2017），美国加州长滩。  \n循环模型通常沿着输入与输出序列的符号位置分解计算过程。通过将位置与计算时间步对齐，它们以前一隐藏状态 ht−1 和位置 t 的输入为函数，生成一系列隐藏状态 ht。这种固有的顺序性阻碍了训练样本内的并行化处理，这在处理较长序列时尤为关键，因为内存限制会制约跨样本的批处理能力。近期研究通过分解技巧[18]和条件计算显著提升了计算效率。\n\n计算[26]，同时在后一种情况下提升了模型性能。然而，顺序计算的根本限制依然存在。\n\n注意力机制已成为各种任务中引人注目的序列建模和转换模型不可或缺的一部分，它允许对依赖关系进行建模，而无需考虑这些依赖在输入或输出序列中的距离[2,16]。然而，除少数情况外[22]，此类注意力机制通常与循环网络结合使用。\n\n在这项工作中，我们提出了Transformer，这是一种摒弃了循环结构、完全依赖注意力机制来捕捉输入与输出之间全局依赖关系的模型架构。Transformer能够实现显著更高的并行化程度，在仅使用八个P100 GPU训练十二小时后，即可在翻译质量上达到新的最优水平。\n\n2 背景\n减少顺序计算的目标也构成了扩展神经GPU[20]、ByteNet[15]和ConvS2S[8]的基础，这些模型均使用卷积神经网络作为基本构建模块，为所有输入和输出位置并行计算隐藏表示。在这些模型中，关联两个任意输入或输出位置信号所需的操作数量随位置间距离的增加而增长：ConvS2S为线性增长，ByteNet为对数增长。这使得学习远距离位置之间的依赖关系变得更加困难[11]。在Transformer中，这一操作数量被减少为常数，尽管这是以因平均注意力加权位置而导致的有效分辨率降低为代价的，我们通过第3.2节中描述的多头注意力机制来抵消这一影响。\n\n自注意力，有时也称为内部注意力，是一种将单个序列的不同位置关联起来的注意力机制，用以计算该序列的表示。自注意力已在多种任务中成功应用，包括阅读理解、抽象摘要、文本蕴含以及学习任务无关的句子表示[4, 22, 23, 19]。  \n端到端记忆网络基于循环注意力机制，而非序列对齐的递归，并已在简单语言问答和语言建模任务中表现出良好性能[28]。  \n然而，据我们所知，Transformer是首个完全依赖自注意力来计算其输入和输出表示的转换模型，而不使用序列对齐的循环神经网络或卷积。在接下来的章节中，我们将描述Transformer模型，阐述自注意力的动机，并讨论其相对于[14, 15]和[8]等模型的优势。  \n\n**3 模型架构**  \n大多数具有竞争力的神经序列转换模型都采用编码器-解码器结构[5, 2, 29]。其中，编码器将符号表示的输入序列（x1, …, xn）映射为连续表示序列 z = (z1, …, zn)。给定 z，解码器随后逐步生成符号的输出序列（y1, …, ym）。在每一步，模型都是自回归的[9]，在生成下一个符号时，将先前生成的符号作为额外输入。  \nTransformer遵循这一整体架构，在编码器和解码器中均使用堆叠的自注意力层和逐点全连接层，分别如图1的左右两部分所示。  \n\n**3.1 编码器和解码器堆栈**\n\n编码器：编码器由N=6个相同的层堆叠而成。每层包含两个子层。第一层是多头自注意力机制，第二层是简单的位置全连接前馈网络。我们在每个子层周围采用残差连接[10]，随后进行层归一化[1]。也就是说，每个子层的输出为LayerNorm(x + Sublayer(x))，其中Sublayer(x)是该子层自身实现的函数。为了便于这些残差连接，模型中的所有子层以及嵌入层均产生维度为d_model=512的输出。\n\n解码器：解码器同样由N=6个相同的层堆叠而成。除了每个编码器层中的两个子层外，解码器还插入了第三个子层，该子层对编码器堆栈的输出执行多头注意力操作。与编码器类似，我们在每个子层周围采用残差连接，随后进行层归一化。我们还修改了解码器堆栈中的自注意力子层，以防止位置关注到后续位置。这种掩码机制，结合输出嵌入偏移一个位置的事实，确保对位置i的预测只能依赖于小于i的已知输出。\n\n3.2 注意力机制  \n注意力函数可描述为将一个查询和一组键值对映射到输出的过程，其中查询、键、值和输出均为向量。输出通过值的加权和计算得出，分配给每个值的权重由查询与对应键的兼容性函数计算。\n\n3.2.1 缩放点积注意力\n\n我们将这种特定的注意力机制称为“缩放点积注意力”（图2）。输入包括维度为dk的查询和键，以及维度为dv的值。我们计算查询与所有键的点积，每个结果除以√dk，然后应用softmax函数以获得值的权重。\n\n在实际操作中，我们同时对一组查询计算注意力函数，将它们打包成矩阵Q。键和值也分别打包成矩阵K和V。我们按以下公式计算输出矩阵：\n\n注意力(Q; K; V) = softmax(QK^T / √dk) V   (1)\n\n最常用的两种注意力函数是加性注意力[2]和点积（乘法）注意力。点积注意力与我们的算法相同，除了缩放因子1/√dk。加性注意力使用具有单个隐藏层的前馈网络计算兼容性函数。尽管两者在理论复杂度上相似，但点积注意力在实践中更快且更节省空间，因为它可以通过高度优化的矩阵乘法代码实现。\n\n对于较小的dk值，两种机制表现相似；但对于较大的dk值，未缩放的加性注意力优于点积注意力[3]。我们推测，对于较大的dk值，点积的幅值会变得很大，将softmax函数推入梯度极小的区域。为了抵消这种影响，我们将点积缩放1/√dk。\n\n**3.2.2 多头注意力**\n\n与其使用d_model维度的键、值和查询执行单一的注意力函数，",
      "mindmap": "```mermaid\nmindmap\n  root((Transformer: Attention Is All You Need))\n    Background & Motivation\n      Traditional Sequence Models (RNN, LSTM)\n      Limitations: Sequential Computation\n    Core Innovation\n      Solely Based on Attention Mechanisms\n      No Recurrence or Convolutions\n    Model Architecture\n      Encoder-Decoder Structure\n      Self-Attention Mechanism\n      Positional Encoding\n    Attention Mechanism\n      Scaled Dot-Product Attention\n      Multi-Head Attention\n    Experiments & Results\n      Machine Translation Tasks (WMT 2014)\n      BLEU Score Improvements\n      High Parallelizability & Reduced Training Time\n```"
    },
    "classification": ""
  },
  {
    "id": "ab319b683f7546759b88498398015115",
    "filename": "Resnet_Deep_Residual_Learning_for_Image_Recognition.pdf",
    "original_name": "Resnet_Deep Residual Learning for Image Recognition.pdf",
    "filepath": "E:\\WEB_TASK\\uploads\\Resnet_Deep_Residual_Learning_for_Image_Recognition.pdf",
    "size": 603123,
    "uploaded_at": "2026-01-02T06:32:17.615789",
    "analysis": {
      "_version": "2",
      "category": "",
      "summary": "该论文的核心贡献是提出残差学习框架，以解决极深神经网络中的性能退化问题。  \n通过让网络层学习输入与输出之间的残差映射，而非直接学习目标映射，极大地优化了训练过程并突破了深度限制。  \n基于此框架构建的ResNet在ImageNet等数据集上取得突破性成果，证明了其通过增加深度持续提升模型性能的有效性。",
      "deep_read": "背景/问题：随着神经网络层数增加，模型性能不升反降，出现训练误差和测试误差同时增大的“性能退化”现象，这并非由梯度消失或爆炸引起（已有方法缓解），而是深度网络本身难以优化。\n\n核心论点：提出“残差学习”框架，让网络层不再直接学习目标映射，而是学习输入与输出之间的残差（即差值），通过恒等跳跃连接将原始输入传递到后续层，使深层网络只需微调而非重构特征。\n\n关键证据：在ImageNet数据集上，152层残差网络比VGG（16层）深8倍但计算复杂度更低；残差网络集成模型将ImageNet测试错误率降至3.57%，赢得ILSVRC 2015分类任务冠军；在CIFAR-10数据集上成功训练出1000层网络，证明框架可稳定支持极深网络训练。\n\n结论或启发：残差结构突破了深度神经网络的深度限制，使模型能通过持续增加层数提升性能，这一设计成为现代深度学习的基石，推动了计算机视觉多项任务的性能飞跃，证明“优化路径设计”比单纯堆叠层数更重要。",
      "translation": "用于图像识别的深度残差学习  \n何恺明 张翔宇 任少卿 孙剑  \n微软研究院  \n{kahe, v-xiangz, v-shren, jiansun}@microsoft.com  \n\n摘要  \n更深的神经网络往往更难训练。我们提出了一种残差学习框架，以简化比以往网络更深层的网络的训练过程。我们明确地将网络层重构为学习关于层输入的残差函数，而非学习无参考的函数。我们提供了全面的实证证据，表明这些残差网络更易于优化，并能够通过显著增加深度来提升准确率。在ImageNet数据集上，我们评估了深度高达152层的残差网络——比VGG网络[40]深8倍，但复杂度更低。这些残差网络的集成模型在ImageNet测试集上取得了3.57%的错误率。该结果在ILSVRC 2015分类任务中获得了第一名。我们还分析了在CIFAR-10数据集上使用100层和1000层网络的表现。  \n\n表征的深度对于许多视觉识别任务至关重要。仅仅得益于我们极深的表征，我们在COCO目标检测数据集上获得了28%的相对性能提升。深度残差网络是我们提交至ILSVRC和COCO 2015竞赛的基础，我们在ImageNet检测、ImageNet定位、COCO检测和COCO分割任务中也均获得了第一名。  \n\n1. 引言  \n深度卷积神经网络[22, 21]已经在图像分类领域取得了一系列突破[21, 49, 39]。深度网络以端到端的多层方式自然地整合了低层/中层/高层特征[49]与分类器，并且特征的“层级”可以通过增加网络深度而得到丰富。\n\n通过堆叠的层数（深度）来衡量。最近的证据[40,43]表明网络深度至关重要，在具有挑战性的ImageNet数据集[35]上取得领先结果的研究[40,43,12,16]均采用了“极深”[40]模型，其深度从十六层[40]到三十层[16]不等。许多其他重要的视觉识别任务[7,11,6,32,27]也显著受益于极深模型。\n\n在深度重要性的推动下，一个问题随之产生：构建更好的网络是否只需简单地堆叠更多层？回答这个问题的一个障碍是著名的梯度消失/爆炸问题[14,1,8]，它从一开始就阻碍了收敛。然而，该问题已通过归一化初始化[23,8,36,12]和中间归一化层[16]得到基本解决，使得数十层的网络能够通过反向传播[22]的随机梯度下降法（SGD）开始收敛。\n\n当更深的网络能够开始收敛时，一个退化问题暴露出来：随着网络深度增加，精度达到饱和（这或许不足为奇），然后迅速下降。出乎意料的是，这种退化并非由过拟合引起，并且向一个足够深的模型添加更多层会导致更高的训练误差。\n\n如[10, 41]所报告并经我们实验充分验证的那样，训练误差会随之增加。图1展示了一个典型示例。\n\n（训练准确率的）退化表明并非所有系统都同样易于优化。我们考虑一个较浅的架构及其对应的更深版本——后者通过添加更多层实现。对于深层模型，存在一种构造性解决方案：增加的层执行恒等映射，其他层则复制已学习的浅层模型参数。这种构造解的存在意味着深层模型的训练误差不应高于其对应的浅层模型。但实验表明，我们现有的求解器无法找到与构造解相当或更优的解决方案（或在可行时间内无法实现）。\n\n本文中，我们通过引入深度残差学习框架来解决退化问题。我们不期望堆叠的若干层直接拟合期望的基础映射，而是显式地让这些层拟合残差映射。形式上，将期望的基础映射表示为H(x)，我们让堆叠的非线性层拟合另一个映射F(x): =H(x)−x。原始映射被重构为F(x)+x。我们假设优化残差映射比优化原始的无参考映射更为容易。极端情况下，若恒等映射是最优解，则将残差推至零比通过堆叠非线性层拟合恒等映射更容易实现。\n\nF(x)+x的公式可通过带有“快捷连接”的前馈神经网络实现（图2）。\n\n快捷连接[2,33,48]是指跳过一层或多层的连接。在我们的设计中，快捷连接仅执行恒等映射，并将其输出与堆叠层的输出相加（图2）。恒等快捷连接既不增加额外参数，也不提高计算复杂度。整个网络仍可通过基于反向传播的随机梯度下降进行端到端训练，并能够使用常见库（如Caffe[19]）轻松实现，无需修改求解器。\n\n我们在ImageNet[35]数据集上进行了全面实验，以展示性能退化问题并评估我们的方法。实验表明：1）我们的极深度残差网络易于优化，但与之对应的“普通”网络（仅简单堆叠层）在深度增加时表现出更高的训练误差；2）我们的深度残差网络能够轻松通过大幅增加深度获得精度提升，产生的结果显著优于以往网络。\n\n在CIFAR-10数据集[20]上也观察到类似现象，这表明优化困难及我们方法的效果并非特定数据集独有的特性。我们在此数据集上成功训练了超过100层的模型，并探索了超过1000层的模型。\n\n在ImageNet分类数据集[35]上，我们通过极深度残差网络取得了优异结果。我们的152层残差网络是ImageNet上迄今最深的网络，同时其复杂度仍低于VGG网络[40]。我们的集成模型在ImageNet测试集上取得了3.57%的top-5错误率，并获得了ILSVRC 2015分类竞赛冠军。这些极深度表征在其他识别任务中也展现出优异的泛化性能，助力我们在后续竞赛中继续取得胜利。\n\n在ILSVRC和COCO 2015竞赛中，于以下任务获得第一名：ImageNet检测、ImageNet定位、COCO检测和COCO分割。这一有力证据表明，残差学习原理具有通用性，我们预计它可应用于其他视觉与非视觉问题。\n\n**2. 相关工作**\n\n**残差表示**。在图像识别中，VLAD[18]是一种通过相对于字典的残差向量进行编码的表示方法，而Fisher Vector[30]可表述为VLAD的概率版本[18]。两者都是用于图像检索和分类的强大浅层表示[4,47]。在向量量化中，编码残差向量[17]已被证明比编码原始向量更有效。\n\n在低级视觉和计算机图形学中，为求解偏微分方程，广泛使用的多重网格法[3]将系统重构为多个尺度下的子问题，其中每个子问题负责处理粗尺度与细尺度之间的残差解。多重网格的替代方法是分层基预条件[44,45]，该方法依赖于表示两个尺度间残差向量的变量。研究证明[3,44,45]，这些求解器的收敛速度远快于未意识到解具有残差特性的标准求解器。这些方法表明，良好的重构或预条件处理能够简化优化过程。\n\n**快捷连接**。关于导致快捷连接的实践与理论[2,33,48]已有长期研究。训练多层感知机的早期实践是添加一个从网络输入连接到输出的线性层[33,48]。在[43,24]中，一些中间层直接连接到辅助分类器，以解决梯度消失/爆炸问题。\n\n中间层直接连接到辅助分类器以解决梯度消失/爆炸问题。[38, 37, 31, 46]等论文提出了通过快捷连接实现层响应、梯度和传播误差中心化的方法。在[43]中，“初始”层由快捷分支和若干更深的分支组成。\n\n与我们的工作同期，“高速公路网络”[41, 42]提出了带门控函数[15]的快捷连接。这些门控函数具有数据依赖性且包含参数，而我们的恒等快捷连接是无参数的。当门控快捷连接“关闭”（趋近于零）时，高速公路网络中的层表示非残差函数。相反，我们的结构始终学习残差函数；恒等快捷连接永不关闭，所有信息始终被传递，同时学习额外的残差函数。此外，高速公路网络未能在深度极大增加（例如超过100层）时展示精度提升。\n\n3. 深度残差学习\n3.1. 残差学习\n假设我们需要若干堆叠层（不一定是整个网络）拟合的基础映射为H(x)，其中x表示这些层中第一层的输入。若假设多个非线性层可以渐近逼近复杂函数²，则等价于假设它们可以渐近逼近残差函数，即H(x)−x（假设输入输出维度相同）。因此我们明确让这些层逼近残差函数F(x): =H(x)−x，而非期望堆叠层直接逼近H(x)。此时原始映射转换为F(x)+x。尽管两种形式都应具备渐近逼近能力，但学习残差形式的优化难度可能有所不同。",
      "mindmap": "```mermaid\nmindmap\n  root((ResNet: Deep Residual Learning for Image Recognition))\n    问题背景\n      深度网络训练困难\n      性能退化问题\n      梯度消失/爆炸问题\n    残差学习框架\n      学习输入与输出的残差映射\n      优化训练过程\n      突破网络深度限制\n    网络架构\n      ResNet设计\n      残差块结构\n      深度可达152层\n    实验结果\n      ImageNet分类任务第一名\n      CIFAR-10上100和1000层测试\n      COCO检测28%相对改进\n    贡献与意义\n      解决极深网络性能退化\n      推动视觉识别任务发展\n      在ILSVRC & COCO竞赛中获胜\n```"
    },
    "classification": ""
  }
]